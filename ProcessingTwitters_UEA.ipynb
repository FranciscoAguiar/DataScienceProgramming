{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProcessingTwitters_UEA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoAguiar/DataScienceProgramming/blob/master/ProcessingTwitters_UEA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WauLBPC8wDZb",
        "outputId": "7e36c7b0-3dae-431b-bdbf-b50c8b677d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!wget http://tiagodemelo.info/datasets/dados-curso-completo.csv.tar.gz\n",
        "!tar -xvf dados-curso-completo.csv.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-04 01:26:38--  http://tiagodemelo.info/datasets/dados-curso-completo.csv.tar.gz\n",
            "Resolving tiagodemelo.info (tiagodemelo.info)... 108.167.188.189\n",
            "Connecting to tiagodemelo.info (tiagodemelo.info)|108.167.188.189|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 143340808 (137M) [application/x-gzip]\n",
            "Saving to: ‘dados-curso-completo.csv.tar.gz’\n",
            "\n",
            "dados-curso-complet 100%[===================>] 136.70M  16.4MB/s    in 9.5s    \n",
            "\n",
            "2020-10-04 01:26:49 (14.4 MB/s) - ‘dados-curso-completo.csv.tar.gz’ saved [143340808/143340808]\n",
            "\n",
            "dados-curso-completo.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4v2ikToDOON",
        "outputId": "0095edb2-3fa1-4fdb-d214-32a24e3060d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('dados-curso-completo.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>usuario</th>\n",
              "      <th>apelido</th>\n",
              "      <th>texto</th>\n",
              "      <th>retweet</th>\n",
              "      <th>seguidores</th>\n",
              "      <th>idioma</th>\n",
              "      <th>lugar</th>\n",
              "      <th>pais</th>\n",
              "      <th>sigla</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-23 18:51:14</td>\n",
              "      <td>Gaboardi</td>\n",
              "      <td>fraslee</td>\n",
              "      <td>@BolsonaroSP Cade o fundão deputado??? Congres...</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>pt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-18 03:18:14</td>\n",
              "      <td>Seu Zé da Farmácia</td>\n",
              "      <td>seuzedafarmacia</td>\n",
              "      <td>Em ação preventiva, o Governo do Estado suspen...</td>\n",
              "      <td>0</td>\n",
              "      <td>56</td>\n",
              "      <td>pt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-29 12:09:27</td>\n",
              "      <td>Marília Oliveira 🇧🇷</td>\n",
              "      <td>Marioliveira_gb</td>\n",
              "      <td>Incrível como os defensores do fim do distanci...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>pt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-04-01 21:11:07</td>\n",
              "      <td>🚩</td>\n",
              "      <td>santosodeto</td>\n",
              "      <td>Fumaça de cigarro pode propagar coronavírus? h...</td>\n",
              "      <td>0</td>\n",
              "      <td>1885</td>\n",
              "      <td>pt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-19 17:49:40</td>\n",
              "      <td>Nova Impressão</td>\n",
              "      <td>novaimpressao_</td>\n",
              "      <td>🚨Todos juntos contra COVID-19💪\\n\\nSolicite seu...</td>\n",
              "      <td>0</td>\n",
              "      <td>124</td>\n",
              "      <td>pt</td>\n",
              "      <td>Brasília</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>BR</td>\n",
              "      <td>-47.8778</td>\n",
              "      <td>-15.77691</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  data              usuario  ... longitude hashtags\n",
              "0  2020-03-23 18:51:14             Gaboardi  ...       NaN       []\n",
              "1  2020-03-18 03:18:14   Seu Zé da Farmácia  ...       NaN       []\n",
              "2  2020-03-29 12:09:27  Marília Oliveira 🇧🇷  ...       NaN       []\n",
              "3  2020-04-01 21:11:07                    🚩  ...       NaN       []\n",
              "4  2020-03-19 17:49:40       Nova Impressão  ... -15.77691       []\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygTXpIYbFPE1",
        "outputId": "5b7eacd7-bb79-4664-8761-a9acb4565139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "data.describe"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of                         data  ...                  hashtags\n",
              "0        2020-03-23 18:51:14  ...                        []\n",
              "1        2020-03-18 03:18:14  ...                        []\n",
              "2        2020-03-29 12:09:27  ...                        []\n",
              "3        2020-04-01 21:11:07  ...                        []\n",
              "4        2020-03-19 17:49:40  ...                        []\n",
              "...                      ...  ...                       ...\n",
              "1658820  2020-03-13 17:26:18  ...                        []\n",
              "1658821  2020-05-12 13:02:34  ...                        []\n",
              "1658822  2020-03-19 18:16:30  ...           ['CORONAVIRUS']\n",
              "1658823  2020-04-07 18:14:45  ...  ['BolsonaroTemRazaoSim']\n",
              "1658824  2020-06-02 16:04:51  ...                        []\n",
              "\n",
              "[1658825 rows x 13 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCv3tsaLTIsm",
        "outputId": "5fae5b55-7cdb-4073-e898-f2e0095c5c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "total_records = len(data.index)\n",
        "print('Total records:', total_records)\n",
        "\n",
        "print('Number of records without date:', data['data'].isnull().sum())\n",
        "print('Number of records without text:', data['texto'].isnull().sum())\n",
        "\n",
        "no_city_count = data['lugar'].isnull().sum()\n",
        "print('Number of records without city:', no_city_count)\n",
        "print('Percentage of records without city:', (no_city_count/total_records)*100)\n",
        "\n",
        "print(data['pais'].isnull().sum())\n",
        "print(data['pais'].isna().sum())\n",
        "print('lat', data['latitude'].isna().sum())\n",
        "print('lat', data['latitude'].isnull().sum())\n",
        "print(data['longitude'].isna().sum())\n",
        "data.info()\n",
        "data.count()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total records: 1658825\n",
            "Number of records without date: 0\n",
            "Number of records without text: 0\n",
            "Number of records without city: 1584220\n",
            "Percentage of records without city: 95.50253944810333\n",
            "1584225\n",
            "1584225\n",
            "lat 1650105\n",
            "lat 1650105\n",
            "1650105\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1658825 entries, 0 to 1658824\n",
            "Data columns (total 13 columns):\n",
            " #   Column      Non-Null Count    Dtype  \n",
            "---  ------      --------------    -----  \n",
            " 0   data        1658825 non-null  object \n",
            " 1   usuario     1658768 non-null  object \n",
            " 2   apelido     1658825 non-null  object \n",
            " 3   texto       1658825 non-null  object \n",
            " 4   retweet     1658825 non-null  int64  \n",
            " 5   seguidores  1658825 non-null  int64  \n",
            " 6   idioma      1658825 non-null  object \n",
            " 7   lugar       74605 non-null    object \n",
            " 8   pais        74600 non-null    object \n",
            " 9   sigla       74595 non-null    object \n",
            " 10  latitude    8720 non-null     float64\n",
            " 11  longitude   8720 non-null     float64\n",
            " 12  hashtags    1658825 non-null  object \n",
            "dtypes: float64(2), int64(2), object(9)\n",
            "memory usage: 164.5+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data          1658825\n",
              "usuario       1658768\n",
              "apelido       1658825\n",
              "texto         1658825\n",
              "retweet       1658825\n",
              "seguidores    1658825\n",
              "idioma        1658825\n",
              "lugar           74605\n",
              "pais            74600\n",
              "sigla           74595\n",
              "latitude         8720\n",
              "longitude        8720\n",
              "hashtags      1658825\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oc0uhpxfaOJ",
        "outputId": "717338cf-cf23-4094-d59a-b98efda3f877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "# instalação da biblioteca de tratamento de texto : spacy\n",
        "#!pip install spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download pt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pt_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2MB 929kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.2.0)\n",
            "Building wheels for collected packages: pt-core-news-sm\n",
            "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp36-none-any.whl size=21186282 sha256=c93419d226b1d643184b986ccd323173ff560a49fd2e0fac7467f49f8f69eaa5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vuxeiuyr/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
            "Successfully built pt-core-news-sm\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lny5yTrogFhL",
        "outputId": "0dc4c025-de16-4e69-dae7-e442c849a9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#como \n",
        "import spacy\n",
        "\n",
        "# Load Portuguese tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load('pt')\n",
        "count = 0\n",
        "df = pd.DataFrame()\n",
        "for row in data.itertuples():\n",
        "    #print(row.texto)\n",
        "    twitter = row.texto\n",
        "    if (twitter.find(\"?\")>=0) :\n",
        "      count +=1    \n",
        "        \n",
        "print(count)\n",
        "\n",
        "  #\n",
        "# Process whole documents\n",
        "text = (\"Caro amigo, A Fumaça de cigarro pode propagar coronavírus?\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "#print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "#print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "#print(\"Split:\", doc.text.split())\n",
        "#print(\"Split mais coerente: \",[token for token in doc])\n",
        "#print(\"String ao inves de token\", [token.orth_ for token in doc])\n",
        "#print(\"pontuação: \",[token.orth_ for token in doc if token.is_punct])\n",
        "\n",
        "#if (\"?\" in [token.orth_ for token in doc if token.is_punct]):\n",
        "#  print('pergunta')\n",
        "   \n",
        "#print( \"Classificacao: \", [(token.orth_, token.pos_) for token in doc])\n",
        "#sentences = [i for i in nlp(text).sents]\n",
        "\n",
        "\n",
        "\n",
        "#print(\"sentences:\", sentences)\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "#for entity in doc.ents:\n",
        "#    print(entity.text, entity.label_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGL8P5YIBMQr",
        "outputId": "851f753c-3d77-47c6-a49f-76047dc97283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKuijMGVVKS"
      },
      "source": [
        "# Visão geral dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MnQh8zaWAVH"
      },
      "source": [
        "\n",
        "\n",
        "1. Devem apresentar um resumo (sumário) com as estatísticas dos dados originais, ou seja, sem\n",
        "qualquer pré-processamento. A apresentação deste tipo de informação relevante para que outras pessoas possam ter uma visão geral dos dados. As estatísticas podem ser apresentadas\n",
        "através de tabelas e/ou gráficos.\n",
        "\n",
        "Total de Registros | Total de Twitters | Total de Cidades | Total de HashTags \n",
        "--- | --- | --- | ---\n",
        "Row 1, Col 1 | Row 1, Col 2 | Row 1, Col 3 | Row 1, Col 4\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Total de Registros Nulos | Total de  | Total de  | Total de  \n",
        "--- | --- | --- | ---\n",
        "Row 1, Col 1 | Row 1, Col 2 | Row 1, Col 3 | Row 1, Col 4\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Inserir Gráfico\n",
        "\n",
        "2. As mensagens foram pré-processadas para que as perguntas (questões) fossem identificadas.\n",
        "Essa coleção de perguntas corresponde ao dataset “DuvidasDB”. A seguir, a equipe deve\n",
        "apresentar as estatísticas sobre esses dados.\n",
        "\n",
        "Total de Perguntas Selecioandas | Total de  | Total de  \n",
        "--- | --- | --- \n",
        "Row 1, Col 1 | Row 1, Col 2 | Row 1, Col 3\n",
        "\n",
        "Inserir Gráfico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohzB_PmSXI6Q"
      },
      "source": [
        "# Temas Discutidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koig2FxKXiXQ"
      },
      "source": [
        "As equipes devem fazer uma análise sobre os temas que são debatidos nas perguntas dos\n",
        "postadas pelos usuários. Faça uma análise se as perguntas são relativas aos seguintes temas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3XONM3vXysA"
      },
      "source": [
        "\n",
        "\n",
        "1.   Doença. Quando a pergunta é relativa à doença. Deve-se observar que a doença é identificada por vários nomes. Exemplo: coronavírus, corona, COVID-19, etc.\n",
        "2.   Medicamento. Quando a pergunta é sobre o uso de determinado medicamento no tratamento\n",
        "da doença.\n",
        "3.   Organizações. Quando a pergunta é relativa a uma determinada entidade ou organização.\n",
        "Emissora de TV, Ministério da Saúde ou empresas, são exemplos de organizações.\n",
        "4.   Pessoas. Quando a pergunta é sobre determinada pessoa. Por exemplo, a pergunta pode\n",
        "ser sobre a atuação que determinado político ou pessoa famosa teve durante esse período de\n",
        "pandemia.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y2xd8fFZTb2"
      },
      "source": [
        "# Visão Temporal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wzYGDzzZaw3"
      },
      "source": [
        "Deve-se fazer uma análise temporal das perguntas que formam o dataset DuvidasDB. Pode-se considerar o intervalo temporal de dias, semanas ou meses. A escolha do intervalo de tempo ficará a cargo das equipes. Exemplos de análise temporal: a) houve um aumento no número de perguntas ao longo do tempo? b) houve uma mudança no perfil das perguntas ao longo do tempo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZyulu0gZwTc"
      },
      "source": [
        "# Visão Geográfica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI87aTDQZ6Gh"
      },
      "source": [
        "Deve-se fazer uma análise geográfica (espacial) das perguntas que formam o dataset DuvidasDB.\n",
        "Existem algumas colunas no dataset que trazem a informação das localizações como, por exemplo,\n",
        "o país, estado e cidade. Em alguns tuítes é possível ainda identificar as coordenadas geográficas de latitude e longitude. Exemplo de análise geográfica: a) os usuários de regiões diferentes fazem perguntas com diferentes focos? Por exemplo, será que os usuários de uma região perguntam mais sobre a doença ou sobre o tratamento? Essa análise ainda pode ser realizada em diversos níveis de\n",
        "área (cidade, estado ou região). Além de apresentar a distribuição das dúvidas dos usuários por região, a equipe deverá fazer uma análise dessa distribuição. Por exemplo, apresentar as razões(ou hipóteses) da ocorrência dessa distribuição."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bpogw7JbZzB"
      },
      "source": [
        ""
      ]
    }
  ]
}